{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-13 00:51:06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda <class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")       \n",
    "else:\n",
    "    device = torch.device(\"cpu\")       \n",
    "\n",
    "def get_variable(x):\n",
    "    return Variable(x).to(device)    \n",
    "    \n",
    "print(device, type(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/marcus/Heisenberg/digit-recognizer/sample_submission.csv',\n",
       " '/home/marcus/Heisenberg/digit-recognizer/test.csv',\n",
       " '/home/marcus/Heisenberg/digit-recognizer/train.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = \"/home/marcus/Heisenberg/digit-recognizer/\"\n",
    "files = [os.path.join(input_path, para) for para in sorted(os.listdir(input_path))]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 785), (28000, 784), (28000, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(files[2])\n",
    "df_test  = pd.read_csv(files[1])\n",
    "df_subms = pd.read_csv(files[0])\n",
    "\n",
    "df_train.shape, df_test.shape, df_subms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28000, 28, 28, 1), numpy.ndarray)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = df_test.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "para.shape, type(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 28, 28, 1), (28000, 28, 28, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df_train.label\n",
    "x_train = df_train.iloc[:,1:].values.reshape(len(df_train),28,28,1)\n",
    "x_test  = df_test.values.reshape(len(df_test),28,28,1)\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test  = x_test.astype(np.float32)\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37800, 28, 28, 1), (4200, 28, 28, 1), (37800,), (4200,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object() takes no parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5c965a71f490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m train_data = Dataset(torch.tensor(x_train),\n\u001b[0;32m----> 9\u001b[0;31m                            torch.LongTensor(y_train.values))\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m train_loader = DataLoader(dataset = train_data,\n",
      "\u001b[0;31mTypeError\u001b[0m: object() takes no parameters"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = Dataset(torch.tensor(x_train),\n",
    "                           torch.LongTensor(y_train.values))\n",
    "\n",
    "train_loader = DataLoader(dataset = train_data,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)\n",
    "\n",
    "val_data = TensorDataset(torch.tensor(x_val),\n",
    "                           torch.LongTensor(y_val.values))\n",
    "val_loader = DataLoader(dataset = val_data,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)   #strides=[1,1]\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型定义-ResNet\n",
    "net = ResNet18().to(device)\n",
    "\n",
    "# 定义损失函数和优化方式\n",
    "#损失函数为交叉熵，多用于多分类问题\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "#优化方式为mini-batch momentum-SGD，并采用L2正则化\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), \n",
    "    lr=LR,              # learning rate\n",
    "    momentum=0.9,       # momentum factor\n",
    "    weight_decay=5e-4   # weight decay (L2 penalty)\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every epochs\"\"\"\n",
    "    lr = LR * (0.1 ** (epoch // 10))\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取在测试集上的准确率\n",
    "def test_net(net, test_loader):  \n",
    "    net.eval()\n",
    "\n",
    "    total = 0\n",
    "    acc_loss = 0.0\n",
    "    acc_correct = 0.0\n",
    "\n",
    "    for inputs, labels in test_loader:      \n",
    "        inputs, labels = get_variable(inputs), get_variable(labels)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        loss = criterion(outputs, labels)        \n",
    "        acc_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        acc_correct += predicted.eq(labels.data).cpu().sum()\n",
    "        \n",
    "    return float(acc_loss)/total, float(acc_correct)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取在测试集上的准确率\n",
    "def pred_net(net, test_loader):  \n",
    "    net.eval()\n",
    "    res = np.array([])\n",
    "    for inputs, labels in test_loader:      \n",
    "        inputs, labels = get_variable(inputs), get_variable(labels)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)        \n",
    "        predicted = predicted.to(\"cpu\") \n",
    "        res = np.hstack([res, predicted])\n",
    "    return res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = {\"train_loss\":[], \"train_acc\":[],\n",
    "        \"val_loss\":[], \"val_acc\":[], \n",
    "        \"epoch\":[], \"lr\":[] }\n",
    "\n",
    "def hist_update(epoch, train_loss, train_acc, val_loss, val_acc, curr_lr):\n",
    "    hist[\"epoch\"].append(epoch)\n",
    "    hist[\"train_loss\"].append(train_loss)\n",
    "    hist[\"train_acc\"].append(train_acc)\n",
    "    hist[\"val_loss\"].append(val_loss)\n",
    "    hist[\"val_acc\"].append(val_acc)\n",
    "    hist[\"lr\"].append(curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "2018-12-12 16:31:26 - [epoch: 1, iter:  60] - Loss: 0.030 | Acc: 29.69% \n",
      "2018-12-12 16:31:32 - [epoch: 1, iter: 120] - Loss: 0.028 | Acc: 34.38% \n",
      "2018-12-12 16:31:38 - [epoch: 1, iter: 180] - Loss: 0.027 | Acc: 38.02% \n",
      "2018-12-12 16:31:44 - [epoch: 1, iter: 240] - Loss: 0.026 | Acc: 41.80% \n",
      "2018-12-12 16:31:50 - [epoch: 1, iter: 300] - Loss: 0.025 | Acc: 44.22% \n",
      "2018-12-12 16:31:56 - [epoch: 1, iter: 360] - Loss: 0.024 | Acc: 47.27% \n",
      "Train loss: 0.012; Train Accuracy：49.63%\n",
      "Validation loss: 0.012; Validation Accuracy：49.17%\n",
      "\n",
      "Epoch: 2\n",
      "2018-12-12 16:32:18 - [epoch: 2, iter: 451] - Loss: 0.017 | Acc: 64.84% \n",
      "2018-12-12 16:32:24 - [epoch: 2, iter: 511] - Loss: 0.017 | Acc: 63.67% \n",
      "2018-12-12 16:32:29 - [epoch: 2, iter: 571] - Loss: 0.016 | Acc: 66.93% \n",
      "2018-12-12 16:32:35 - [epoch: 2, iter: 631] - Loss: 0.016 | Acc: 67.77% \n",
      "2018-12-12 16:32:41 - [epoch: 2, iter: 691] - Loss: 0.016 | Acc: 67.66% \n",
      "2018-12-12 16:32:47 - [epoch: 2, iter: 751] - Loss: 0.015 | Acc: 68.88% \n",
      "Train loss: 0.009; Train Accuracy：61.64%\n",
      "Validation loss: 0.009; Validation Accuracy：59.55%\n",
      "\n",
      "Epoch: 3\n",
      "2018-12-12 16:33:09 - [epoch: 3, iter: 842] - Loss: 0.016 | Acc: 67.97% \n",
      "2018-12-12 16:33:15 - [epoch: 3, iter: 902] - Loss: 0.013 | Acc: 75.78% \n",
      "2018-12-12 16:33:21 - [epoch: 3, iter: 962] - Loss: 0.012 | Acc: 76.04% \n",
      "2018-12-12 16:33:27 - [epoch: 3, iter:1022] - Loss: 0.012 | Acc: 76.37% \n",
      "2018-12-12 16:33:33 - [epoch: 3, iter:1082] - Loss: 0.012 | Acc: 77.19% \n",
      "2018-12-12 16:33:39 - [epoch: 3, iter:1142] - Loss: 0.011 | Acc: 77.21% \n",
      "Train loss: 0.005; Train Accuracy：79.46%\n",
      "Validation loss: 0.006; Validation Accuracy：75.54%\n",
      "Validation accuracy imporoved from 60.00% to 75.54%, Model saved.\n",
      "\n",
      "Epoch: 4\n",
      "2018-12-12 16:34:01 - [epoch: 4, iter:1233] - Loss: 0.011 | Acc: 74.22% \n",
      "2018-12-12 16:34:07 - [epoch: 4, iter:1293] - Loss: 0.010 | Acc: 78.52% \n",
      "2018-12-12 16:34:12 - [epoch: 4, iter:1353] - Loss: 0.009 | Acc: 80.21% \n",
      "2018-12-12 16:34:18 - [epoch: 4, iter:1413] - Loss: 0.009 | Acc: 79.88% \n",
      "2018-12-12 16:34:24 - [epoch: 4, iter:1473] - Loss: 0.009 | Acc: 80.00% \n",
      "2018-12-12 16:34:30 - [epoch: 4, iter:1533] - Loss: 0.009 | Acc: 80.60% \n",
      "Train loss: 0.004; Train Accuracy：82.58%\n",
      "Validation loss: 0.005; Validation Accuracy：77.10%\n",
      "Validation accuracy imporoved from 75.54% to 77.10%, Model saved.\n",
      "\n",
      "Epoch: 5\n",
      "2018-12-12 16:34:52 - [epoch: 5, iter:1624] - Loss: 0.008 | Acc: 82.03% \n",
      "2018-12-12 16:34:58 - [epoch: 5, iter:1684] - Loss: 0.008 | Acc: 85.16% \n",
      "2018-12-12 16:35:04 - [epoch: 5, iter:1744] - Loss: 0.008 | Acc: 85.42% \n",
      "2018-12-12 16:35:10 - [epoch: 5, iter:1804] - Loss: 0.008 | Acc: 85.94% \n",
      "2018-12-12 16:35:16 - [epoch: 5, iter:1864] - Loss: 0.008 | Acc: 85.31% \n",
      "2018-12-12 16:35:22 - [epoch: 5, iter:1924] - Loss: 0.008 | Acc: 84.64% \n",
      "Train loss: 0.003; Train Accuracy：84.56%\n",
      "Validation loss: 0.005; Validation Accuracy：78.22%\n",
      "Validation accuracy imporoved from 77.10% to 78.22%, Model saved.\n",
      "\n",
      "Epoch: 6\n",
      "2018-12-12 16:35:44 - [epoch: 6, iter:2015] - Loss: 0.007 | Acc: 84.38% \n",
      "2018-12-12 16:35:49 - [epoch: 6, iter:2075] - Loss: 0.007 | Acc: 86.33% \n",
      "2018-12-12 16:35:55 - [epoch: 6, iter:2135] - Loss: 0.005 | Acc: 89.06% \n",
      "2018-12-12 16:36:01 - [epoch: 6, iter:2195] - Loss: 0.005 | Acc: 90.04% \n",
      "2018-12-12 16:36:07 - [epoch: 6, iter:2255] - Loss: 0.006 | Acc: 88.59% \n",
      "2018-12-12 16:36:13 - [epoch: 6, iter:2315] - Loss: 0.006 | Acc: 87.50% \n",
      "Train loss: 0.006; Train Accuracy：75.82%\n",
      "Validation loss: 0.008; Validation Accuracy：70.51%\n",
      "\n",
      "Epoch: 7\n",
      "2018-12-12 16:36:35 - [epoch: 7, iter:2406] - Loss: 0.005 | Acc: 85.16% \n",
      "2018-12-12 16:36:41 - [epoch: 7, iter:2466] - Loss: 0.006 | Acc: 83.98% \n",
      "2018-12-12 16:36:47 - [epoch: 7, iter:2526] - Loss: 0.007 | Acc: 82.29% \n",
      "2018-12-12 16:36:53 - [epoch: 7, iter:2586] - Loss: 0.007 | Acc: 84.57% \n"
     ]
    }
   ],
   "source": [
    "soa_acc = 0.6\n",
    "soa_epoch = 0\n",
    "\n",
    "for epoch in range(pre_epoch, EPOCH):\n",
    "    print('\\nEpoch: %d' % (epoch + 1))\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # 准备数据\n",
    "        length = len(train_loader)\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%60==59: # 每训练60个batch打印一次loss和准确率\n",
    "            sum_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.data).cpu().sum()\n",
    "            \n",
    "            train_loss = sum_loss / (i + 1)\n",
    "            train_acc = float(correct) / total\n",
    "            \n",
    "            print(time.strftime(\"%Y-%m-%d %H:%M:%S\")+\" - \"+'[epoch:%2d, iter:%4d] - Loss: %.03f | Acc: %.2f%% '\n",
    "                  % (epoch + 1, (i + 1 + epoch * length), train_loss, 100. * train_acc))\n",
    "        \n",
    "    # 每训练完一个epoch测试一下准确率\n",
    "    train_loss, train_acc = test_net(net, train_loader)\n",
    "    print('Train loss: %.3f; Train Accuracy：%.2f%%' % (train_loss, 100*train_acc))\n",
    "    val_loss, val_acc = test_net(net, test_loader)\n",
    "    print('Validation loss: %.3f; Validation Accuracy：%.2f%%' % (val_loss, 100*val_acc))\n",
    "\n",
    "    if val_acc > soa_acc:\n",
    "        print(\"Validation accuracy imporoved from %.2f%% to %.2f%%, Model saved.\" % (100.*soa_acc, 100.*val_acc))\n",
    "        torch.save(net.state_dict(), 'model/net_resnet34_best.pth')\n",
    "        soa_acc = val_acc\n",
    "\n",
    "    # 返回当前的Learning Rate & 并为下一个epoch调整Learning Rate\n",
    "    para = optimizer.param_groups[0]\n",
    "    curr_lr = para[\"lr\"]\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "    hist_update(epoch, train_loss, train_acc, val_loss, val_acc, curr_lr)\n",
    "\n",
    "print(\"\\nTraining Finished, Total_Epoch: %d\" % EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(hist)\n",
    "hist.plot(x=\"epoch\", y=[\"train_loss\",\"val_loss\"])\n",
    "hist.plot(x=\"epoch\", y=[\"train_acc\",\"val_acc\"])\n",
    "hist.plot(x=\"epoch\", y=[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 13 00:52:06 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.77                 Driver Version: 390.77                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 107...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   32C    P8    15W / 180W |    437MiB /  8119MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1699      G   /usr/lib/xorg/Xorg                           195MiB |\r\n",
      "|    0      1918      G   /usr/bin/gnome-shell                         229MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "?TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
